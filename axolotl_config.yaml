base_model: HuggingFaceTB/SmolLM2-360M-Instruct

datasets:
  - path: muellerzr/text_to_terminal_v2
    split: train
    type: oasst
    # Optional: choose who to compute loss on (assistant replies only)
    roles_to_train: ["assistant"]
    train_on_inputs: false
    train_on_eos: turn

val_set_size: 0.02

seed: 42
num_epochs: 3
micro_batch_size: 64
gradient_accumulation_steps: 1
sequence_len: 2048
sample_packing: false
pad_to_sequence_len: false
train_on_inputs: false

optimizer: adamw_torch
learning_rate: 2.0e-4
weight_decay: 0.0
lr_scheduler: cosine
warmup_ratio: 0.03

bf16: true
fp16: false
gradient_checkpointing: true
logging_steps: 25

output_dir: ./outputs/smollm2-text2terminal-strict
save_steps: 200
save_total_limit: 2
eval_steps: 200
eval_strategy: steps
